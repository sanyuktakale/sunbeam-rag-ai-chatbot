{
    "course_name": "Dream LLM",
    "url": "https://sunbeaminfo.in/modular-courses/dreamllm-training-institute-pune",
    "general_info": {
        "summary": "Course Name : Dream LLM | Batch Schedule : 05-Jan-2026   To   09-Feb-2026 | Schedule : Monday - Thursday | Duration : 5 weeks | Timings : 9:00 PM  To  11:00 PM | Fees : Rs. INR 20000/- 12000/-(Inc.18% GST)"
    },
    "sections": [
        {
            "title": "Prerequisite:",
            "content": "- Basic to intermediate Python programming (loops, functions, lists, dictionaries, basics of classes)\n- Basic understanding of ML concepts (what a model, training, and loss mean)\n- High-level math intuition: addition, subtraction, multiplication and division    \n- Familiarity with PyTorch or NumPy is helpful but not mandatory\n- A laptop with 8 GB RAM (GPU recommended but not required)\n- Curiosity and willingness to learn how LLMs work internally (not just use APIs)                                                \n                                                Click to Register"
        },
        {
            "title": "Syllabus:",
            "content": "Introduction to LLM\n\t\n\t\tWhat is large language model?\n\t\tTypes of LLM\n\t\tApplications of LLM\n\t\tRequirements of DreamLLM\n\t\n\t\n\tMathematics and Pytorch required for LLM\n\t\n\t\tVectors\n\t\tMatrices\n\t\tTensors\n\t\tLinear Algebra\n\t\tAll maths topics with Pytorch implementation\n\t\n\t\n\tIntroduction to Deep Learning\n\t\n\t\tDifference between ML and DL\n\t\tNeural network architecture\n\t\tIntroduction to forward and backward propagation\n\t\n\t\n\tWhy should you build DreamLLM?\n\t\n\t\tStrategies of building LLM\n\t\tFine tuning vs creating custom LLM\n\t\tSupervised (labelled) vs unsupervised (unlabelled) training custom LLM\n\t\tArchitecture of DreamLLM\n\t\n\t\n\tTransformers\n\t\n\t\tRelationship between LLM and transformer\n\t\tIntroduction to transformers\n\t\tTransformer architecture\n\t\tTypes of transformers\n\t\n\t\n\tUnderstanding data\n\t\n\t\tData requirements to train DreamLLM\n\t\tPreprocessing the data\n\t\tTokenization\n\t\t\n\t\t\tCustom tokenization\n\t\t\tEmbeddings basics\n\t\t\tByte pair encoding\n\t\t\n\t\t\n\t\n\t\n\tProcessing data using sequence modelling\n\t\n\t\tIntroduction to RNN (LSTM and GRU)\n\t\tIntroduction and requirement of attention mechanism\n\t\tImplementing attention mechanism\n\t\t\n\t\t\tSelf attention mechanism\n\t\t\tSingle head\n\t\t\tMulti head\n\t\t\n\t\t\n\t\n\t\n\tCreating LLM from scratch\n\t\n\t\tDeciding the layers, activation functions\n\t\tImplementing feed forward network\n\t\tGenerating text using custom LLM\n\t\n\t\n\tFine tuning for further tasks\n\t\n\t\tFine tuning the DreamLLM on labelled data\n\t\tFine tuning the DreamLLM on unlabelled data\n\t\tEvaluation of DreamLLM\n\t\n\t\n\tDeploying the DreamLLM\n\t\n\t\tUse AWS cloud for deployment\n\t\tAutomate the deployment using AWS DevOps tools\n\t\n\t\n                                                \n                                                Click to Register"
        },
        {
            "title": "Outcomes:",
            "content": "Academic Outcomes\n\n\n\tTheoretical Understanding\n\n\t\n\t\tDeep knowledge of mathematical foundations (Linear Algebra, Probability & Statistics, Optimization).\n\t\tUnderstanding key concepts in Machine Learning and Deep Learning.\n\t\tIn-depth knowledge of Transformer architectures, Attention Mechanisms, and their variants (GPT, BERT, T5).\n\t\n\t\n\tPractical Skills\n\t\n\t\tProficiency in Python and the use of deep learning frameworks like TensorFlow and PyTorch.\n\t\tAbility to implement, train, and fine-tune advanced neural network models.\n\t\tSkills in data preprocessing, feature extraction, and handling large datasets.\n\t\n\t\n\tEvaluation and Deployment\n\t\n\t\tKnowledge of evaluation metrics for NLP models (BLEU, ROUGE).\n\t\tTechniques for deploying LLMs in production environments using APIs and containerization tools (Docker, Kubernetes).\n\t\n\t\n\tProfessional Outcomes\n\t\n\t\tJob Readiness\n\t\t\n\t\t\tEnhanced employability in roles related to AI research, machine learning engineering, and NLP development.\n\t\t\tAbility to tackle complex problems in natural language processing and develop innovative solutions\n\t\t\n\t\t\n\t\tResearch Capabilities - Skills to contribute to the field of AI and NLP through original research. - Understanding of current trends, challenges, and opportunities in LLM development. - Understanding of ethical considerations in AI and NLP, including bias mitigation, privacy, and data governance. - Commitment to developing AI solutions that are fair, transparent, and responsible.\n\t\n\t\n                                                \n                                                Click to Register"
        },
        {
            "title": "Capstone project:",
            "content": "Project Overview\n\n\n\tProject Name: DreamLLM\n\n\t\n\t\tObjective: Develop a state-of-the-art Large Language Model (LLM) from scratch, focusing on innovation and advanced capabilities.\n\t\tScope: The project aims to create an LLM with the ability to understand, generate, and interpret human language.\n\t\tTarget Audience: Researchers, developers, and enthusiasts interested in advanced AI and NLP.\n\t\tNote: For the sake of time and infrastructure, the LLM will be trained on the limited data. (For the real version of it, you may want to go for huge data which will cost more.)\n\t\n\t\n\tTechnical Requirements\n\t\n\t\tProgramming Language: Python (version 3.11)  Deep Learning Frameworks:PyTorch\n\t\tSoftware Tools:\n\t\t\n\t\t\tJupyter Notebook\n\t\t\tGit for version control\n\t\t\tDocker for containerization (optional, depending on the participants)\n\t\t\tKubernetes for deployment (optional, depending on scalability needs)\n\t\t\n\t\t\n\t\tLibraries and Tools:\n\t\t\n\t\t\tNumpy, Pandas (for data manipulation)\n\t\t\tNLTK or SpaCy (for text preprocessing)\n\t\t\tMatplotlib, Seaborn (for data visualization)\n\t\t\n\t\t\n\t\tHardware Requirements:\n\t\t\n\t\t\tOwn infrastructure\n\t\t\t\n\t\t\t\tHigh-performance GPU(s)\n\t\t\t\tSufficient storage for large datasets\n\t\t\t\tAdequate RAM (minimum 32 GB)???????\n\t\t\t\n\t\t\t\n\t\t\tGoogle Colab with TPU or GPU support\n\t\t\tRunpod\n\t\t\n\t\t\n\t\tFunctional Requirements\n\t\t\n\t\t\tModel Architecture:\n\t\t\t\n\t\t\t\tTransformer-based architecture with advanced attention mechanisms\n\t\t\t\tSupport for different variants (e.g., GPT, BERT, T5)\n\t\t\t\tPre-training and fine-tuning capabilities\n\t\t\t\n\t\t\t\n\t\t\tData Handling:\n\t\t\t\n\t\t\t\tData preprocessing pipeline (tokenization, embedding)\n\t\t\t\tLarge dataset support (e.g., Wikipedia, Common Crawl)\n\t\t\t\n\t\t\t\n\t\t\tTraining Process:\n\t\t\t\n\t\t\t\tDistributed training for scalability\n\t\t\t\tHyperparameter tuning (learning rate, batch size, etc.)\n\t\t\t\n\t\t\t\n\t\t\tEvaluation Metrics:\n\t\t\t\n\t\t\t\tLanguage accuracy metrics (BLEU, ROUGE)\n\t\t\t\tPerformance benchmarks (e.g., GLUE tasks)\n\t\t\t\n\t\t\t\n\t\t\tInference and Deployment:\n\t\t\t\n\t\t\t\tAPI-based deployment (RESTful or gRPC)\n\t\t\t\tReal-time inference capabilities\n\t\t\t\n\t\t\t\n\t\t\tUser Interface:\n\t\t\t\n\t\t\t\tWeb-based interface for interacting with the model (optional)\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n                                                \n                                                Click to Register"
        },
        {
            "title": "Video recording will be available till 05 June 2026 on portal",
            "content": "Click to Register"
        },
        {
            "title": "Dream LLM — Course Overview",
            "content": "Click to Register"
        }
    ],
    "batch_schedule_table": [
        {
            "Sr.No": "1",
            "Batch Code": "DreamLLM-O-01",
            "Start Date": "05-Jan-2026",
            "End Date": "09-Feb-2026",
            "Time": "9:00 PM  To  11:00 PM"
        }
    ]
}